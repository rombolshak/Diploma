\section{Обработка полученного ключа}
Любой протокол квантового распределения частей состоит из двух частей: квантовой и классической \cite{gisin2002Quacry}. Квантовая часть включает в себя собственно передачу квантовых состояний, манипуляции над ними и измерения, и проводится с использованием квантового канала связи. Классическая часть проводится через открытый аутентичный классический канал (то есть такой, который злоумышленник может прослушивать, но не может изменить передаваемые данные) и включает в себя коррекцию ошибок и усиление секретности. 

\subsection{Коррекция ошибок}
Результатом квантовой части является так называемый <<сырой>> ключ на обеих сторонах квантового канала. Этот ключ должен быть очищен от всех ошибок, неизбежно возникающих в процессе. Ошибки могут быть как внутренними (из-за низкой эффективности самого протокола), так и внешними, исходящими как от физических недостатков используемой аппаратуры, так и от действий подслушивателя. Внутренние ошибки исправить проще, что обычно и делается по ходу протокола путем передачи через открытый канал информации о начальном состоянии. К примеру, в протоколе BB84\cite{bb84} половина от всей переданной последовательности состояний будет измерена Бобом в базисе, не совпадающим с тем, в каком их готовила Алиса. Это приведет к тому, что 50\% полученных бит в итоге не войдут в секретный ключ. В реальности же, оставшиеся биты все еще будут подвержены ошибкам, возникших либо со стороны аппаратуры, либо из-за действий злоумышленника, что в принципе неразличимо. В дальнейшем будет предполагаться, что используемая аппаратура идеальна, 
а все ошибки возникают исключительно из-за воздействия Евы.

В \cite{information_reconcilation} продемонстрировано существование оптимального, хоть и не эффективного, протокола, оставляющего минимальное количество информации подслушивателю. На практике же стандартом де-факто для всех протоколов квантового распределения ключей является протокол коррекции ошибок \textit{Cascade}.

\subsubsection{Протокол Cascade}\label{sec:cascade_description}
\begin{enumerate}
  \item Работа протокола происходит в несколько проходов, число которых определяется сторонами до начала процесса. У каждой из сторон имеется своя битовая строка: $A = A_1,\dots,A_n$ и $B = B_1,\dots,B_n$ (где $B_i,A_i \in \{0, 1\}$) у Алисы и Боба соответственно.
  
  \item На каждом проходе $i$ Алиса и Боб выбирают значение $k_i$ и случайную функцию $f_i: [1..n]\rightarrow[1..\ceil{\frac{n}{k_i}}]$. Биты, чья позиция находится в множестве $K^i_j = \{l~|~f_i(l) = j\}$ формируют блок $j$ в проходе $i$. Таким образом, строка каждого из участников перемешивается случайно выбранным образом и разбивается на блоки размера $k_i$.
  
  \item Алиса посылает Бобу четности каждого блока текущего прохода: 
  $$ a_j = \bigoplus_{l \in K^i_j} A_l,~ 1 \leq j \leq \ceil[\Big]{\frac{n}{k_i}} $$
  
  \item Боб вычисляет свои $b_j$ таким же образом и сравнивает их с полученными $a_j$. 
  
  \item Множество блоков, в которых содержится нечетное число ошибок, обозначим $\mathcal{E}$.
  Изначально в это множество заносятся блоки, четности которых не совпали: ($b_j \neq a_j $).
  
  \item\label{while_condition} Если $\mathcal{E} \neq \emptyset$, выбирается блок наименьшего размера из $\mathcal{E}$, иначе проход считается завершенным.
  
  \item Для выбранного блока проводится дихотомический поиск ошибки:
  \begin{enumerate}
    \item Алиса посылает Бобу четность первой половины бит указанного блока.
    \item Боб сравнивает полученные данные со своими. Если четности не совпали, то в первой половине блока находится нечетное число ошибок; если совпали~--- во второй.
    \item Процесс повторяется с той половиной блока, о которой теперь известно, что в ней находится нечетное число ошибок.
    \item В конце концов будет найдена одна позиция, значения строк в которой у Алисы и Боба различаются.
  \end{enumerate}
  
  \item В результате поиска Боб обнаружит позицию $l$ такую, что $B_l \neq A_l$ и исправит свое значение. 
  
  \item Все блоки $K^u_v$ для $1 \leq u < i$ такие, что $l \in K^u_v$ (то есть содержащие в себе только что исправленную позицию) будут теперь иметь нечетное число ошибок. Обозначим множество этих блоков за $\mathcal{K}$
  
  \item Множество $\mathcal{E}$ изменяется следующим образом: для каждого блока $K \in \mathcal{K}$, если $K \in \mathcal{E}$, происходит его удаление из $\mathcal{E}$, в противном случае блок $K$ добавляется к $\mathcal{E}$. Формально говоря, $\mathcal{E}' = \mathcal{E} \bigtriangledown \mathcal{K} = (\mathcal{E} \cup \mathcal{K}) \setminus (\mathcal{E} \cap \mathcal{K})$.
  
  \item Дальнейшее выполнение продолжается с шага \ref{while_condition} и множеством $\mathcal{E}'$ в качестве $\mathcal{E}$.

\end{enumerate}
На практике обычно используется 4 прохода, размеры блоков в каждом следующем проходе удваиваются, а начальный размер блока выбирается из соображений максимизации количества исправленных ошибок в первом проходе, что зависит от предполагаемой величины ошибок $p$.

\subsection{Усиление секретности}
После коррекции ошибок Алиса и Боб имеют одинаковую строку $W$ из $n$ бит, о которой подслушивателю Еве известна некоторая информация, описываемая вероятностным распределением $P_{W|V=v}$ над всеми $n$-битными строками, где $v$ обозначает конкретное значение случайной величины $V$, представляющей собой сумму всей её (Евы) информации. Для примера, Ева может узнать некоторые биты или косвенную информацию о них во время квантовой части протокола, некоторые во время исправления ошибок (четности блоков) или найти какой-то более изощренный способ узнать некоторую информацию о строке $W$. Алиса и Боб имеют представление о том, сколько примерно информации может быть доступно злоумышленнику, то есть о распределении $P_{W|V=v}$, но они не знают наверняка, к каким битам их строки относится эта информация. Используя открытый аутентичный канал (свободный для прослушивания, но сообщения в котором невозможно модифицировать или несанкционированно вставить), они хотят договориться о такой функции $g: \{0, 1\}^n \rightarrow \{
0,1\}^r$, что Ева, несмотря на ее частичное знание о строке $W$ и полном знании о функции $g$, не будет знать практически ничего о $g(W)$. Этот процесс преобразует частично секретную $n$-битную строку $W$ в высокосекретную, но более короткую, $r$-битную строку $g(W)$, которая уже может использоваться как секретный ключ.

Метод, которым выбирается функция $g$, был предложен Беннетом в работе \cite{privacy_amplification_by_public_discussion} и заключается в выборе конкретной функции случайным образом из заранее известного универсального семейства хеш-функций (введены Картером и Вегманом в \cite{universal_hashing}), отображающих $n$-битные строки в $r$-битные.

\begin{definition}
  Семейство $\mathcal{F}$ функций $\mathcal{A}\rightarrow\mathcal{B}$ называется \textit{универсальным}, если 
  $$  \pr[f(x_1) = f(x_2)] < \frac{1}{|B|} ~~\forall x_1, x_2 \in \mathcal{A}: x_1 \neq x_2, $$ а $f$ выбирается из $\mathcal{F}$ в соответствии с равномерным распределением.
\end{definition}

В \cite{privacy_amplification} было показано, что энтропия Реньи (определение ниже) распределения вероятности Евы о строке $W$ дает нижнюю границу размера $r$ секретного ключа, который возможно получить из $W$ с помощью универсального хеширования.

\begin{definition}
  Пусть $X$~--- случайная величина из алфавита $\mathcal{X}$ с распределением вероятности $P_X$. \textit{Вероятностью коллизии} $P_c(X)$ называется вероятность того, что $X$ примет одно и то же значение дважды в двух независимых экспериментах:
  \begin{equation}
    P_c(X) = \sum_{x\in X} P_X(x)^2.
  \end{equation}
\end{definition}

\begin{definition}
  \textit{Энтропия Реньи порядка 2} (или просто <<энтропия Реньи>>) случайной величины $X$ определяется как 
  \begin{equation}
    R(X) = -\log P_c(X).
  \end{equation}
\end{definition}

\begin{definition}
  Условная энтропия Реньи $R(X|Y)$ определяется как
  \begin{equation}
    R(X|Y) = \sum_y P_Y(y) R(X|Y=y) = -\sum_y P_Y(y) \log P_c(X|y).
  \end{equation}
\end{definition}

Иначе говоря, $R(X)$ может быть выражена как $R(X) = -\log E[P_X(X)]$, где $E[\cdot]$ обозначает математическое ожидание. Энтропия Шеннона $H(X)$ аналогичным образом выражается как $H(X) = -E[\log P_X(X)]$. Из неравенства Йенсена (см \cite{jensens_inequality}) очевидным образом следует следующее утверждение.

\begin{statement}
  Энтропия Реньи ограничена сверху энтропией Шеннона:
  $$ R(X) \leq H(X), $$
  причем равенство достигается тогда и только тогда, когда $P_X$ является равномерным распределением над алфавитом $\mathcal{X}$ или его подмножеством.
\end{statement}

Аналогично имеем $H(X|Y) \geq R(X|Y)$. Следует заметить, что энтропия Реньи, как и энтропия Шеннона, всегда положительна.

Следующая теорема является основным результатом, полученным в работе \cite{privacy_amplification}:

\begin{theorem}
  Пусть $X$~--- случайная величина в алфавите $\mathcal{X}$ с вероятностым распределением $P_X$ и энтропией Реньи $R(X)$. Кроме того, пусть $G$~--- случайная величина, отвечающая случайному выбору (внутри равномерного распределения) члена универсального семейства хеш-функций, отображающих $\mathcal{X} \rightarrow \{0,1\}^r$. Тогда
  \begin{equation}
    H(G(X)|G) \geq R(G(X)|G) \geq r - \frac{2^{r - R(X)}}{\ln 2}.
  \end{equation}
\end{theorem}

Необходимо обратить внимание, что $G$ является случайной величиной, и что энтропия $H(G(X)|G)$ есть среднее по всем возможным выборам функции $g$. Может случиться так, что $H(G(X)|G=g) = H(g(X))$ значительно отличается от $r$ для некоторой функции $g$, но такая функция $g$ может быть выбрана лишь с пренебрежимо малой вероятностью.

Эта теорема применяется и к условным вероятностным распределениям, таким как описанное выше $P_{W|V=v}$. Если известно, что энтропия Реньи информации Евы $R(W|V=v)$ составляет как минимум $t$ бит, и Алиса с Бобом выбирают $S = G(W)$ как их секретный ключ, тогда
\begin{equation}
  R(S|G,V=v) = R(G(W)|G,V=v) \geq r - \frac{2^{r - t}}{\ln 2}.
\end{equation}

Величина $H(S|G,V=v)$ имеет следующий информационный смысл: это количество бит, котороe не хватает Еве, чтобы полностью узнать ключ $S$, если она имеет конкретную информацию о ключе $v$, в среднем по всем хеш-функциям $g$ универсального семейства $G$.

Ключ $S$ будет действительно практически секретным, так как $H(S|G,V=v) \geq R(S|G,V=v)$, и, следовательно, $H(S|G,V=v)$ сколько угодно близко к максимальному значению. Более точно, если $r < t$, то общая информация Евы о ключе $S$ уменьшается экспоненциально с фактором $t-r$.





